{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fecacc90",
   "metadata": {},
   "source": [
    "# PyTorch tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf8e6c",
   "metadata": {},
   "source": [
    "This notebook serves as a hands-on ***tutorial*** designed for individuals who want to explore and better understand ***PyTorch***. In particular, we will focus on its application in ***computer vision***, demonstrating how to build and train models for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809a79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e7663",
   "metadata": {},
   "source": [
    "**Most important points to explore :**\n",
    "\n",
    "- Loading Data and Devices\n",
    "- Autograd\n",
    "- Optimizer\n",
    "- Loss\n",
    "- Model\n",
    "- DataLoader\n",
    "- Split\n",
    "- Saving/Loading Weights\n",
    "- Evaluation\n",
    "- Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bfb1ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96740a",
   "metadata": {},
   "source": [
    "# Importing dataset\n",
    "\n",
    "In this notebook, we will use the **FashionMNIST** dataset available in the **torchvision** package. This dataset consists of grayscale images of Zalando’s fashion articles, such as shoes, shirts, and bags. It is commonly used as a benchmark for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8232d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=T.ToTensor(), \n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa2eac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626052bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ffc682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3962fc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_idx = torch.randint(0, len(train_data), (1,))[0]\n",
    "img_tensor, label = train_data[img_idx]\n",
    "to_pil = T.ToPILImage()\n",
    "img = to_pil(img_tensor)\n",
    "class_name = class_names[label]\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(f\"Classe : {class_name}\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755bc59",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2d672",
   "metadata": {},
   "source": [
    "In deep learning, we typically use the **mini-batch gradient descent** algorithm.\n",
    "Therefore, **DataLoaders** are designed to efficiently organize datasets into batches, making it easier to feed data to the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68cc881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataloader: 1875 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {batch_size}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217ff58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3798d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7f23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82043e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ecd94f",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef56583",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are one of the fundamental architectures in computer vision. A typical CNN architecture is composed of ***two main parts***:\n",
    "\n",
    "1. ***Convolutional layers*** – These layers are responsible for feature extraction.\n",
    "They apply convolutional filters to the input image to detect patterns such as edges, textures, or more complex shapes in deeper layers.\n",
    "\n",
    "2. ***Fully connected layers*** – These layers act as the “classifier” part of the network.\n",
    "They take the high-level features extracted by the convolutional layers and use them to make predictions, such as identifying the class of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b800a",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"pytorch_cnn_lenet.jpg\" width=\"750\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970de9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_cnn(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Model_cnn, self).__init__()\n",
    "\n",
    "\n",
    "        # Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1 , out_channels = 20 , kernel_size = 3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 20 , out_channels = 50 , kernel_size = 3)\n",
    "        # self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(50**, 500)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "        out = self.conv1(image)\n",
    "        out = self.pool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.pool1(out)\n",
    "\n",
    "        out = torch.flatten(out, 1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea95d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8886463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb8bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b48031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf108b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af3de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffc446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
